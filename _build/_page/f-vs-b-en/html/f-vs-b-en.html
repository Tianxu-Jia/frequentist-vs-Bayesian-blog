
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Tianxu Jia&#39;s Blog</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-2 bd-sidebar site-navigation show single-page" id="site-navigation">
    
</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            
            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/f-vs-b-en.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="frequentist-vs-bayesian">
<h1>Frequentist vs Bayesian<a class="headerlink" href="#frequentist-vs-bayesian" title="Permalink to this headline">¶</a></h1>
<p style="color:blue"> Tianxu Jia</p>
<p style="color:blue">Station10 Ltd</p><a class="bg-primary mb-1 reference internal image-reference" href="_images/Bayesian-vs-frequentist.png"><img alt="_images/Bayesian-vs-frequentist.png" class="bg-primary mb-1 align-center" src="_images/Bayesian-vs-frequentist.png" style="width: 500px;" /></a>
<div class="section" id="the-problem-formulation">
<h2>1. The problem formulation<a class="headerlink" href="#the-problem-formulation" title="Permalink to this headline">¶</a></h2>
<p>In a marketing campaign, a retailer sends a coupon to the customer. How likely is it that the customer will buy the product? Maybe we should give it a shot since it won’t cost us a lot. Statistically, we can make some experiments, estimate the probability, and then decide how to mark targets based on the results. Global warming is a big topic that is relevant to all people on the planet.  As we are an independent thinker, not the ones who follow the crowd, we would like to ask: Is it possible to still do some random experiments for the conclusion? Maybe it is absolutely impossible.</p>
<p>We need impeccable logic and support from data!</p>
<p>Generally speaking, there are two schools of thought in the field of data science and machine learning: the Bayesian and the frequentist. By analyzing their basic assumptions, this blog will explain the theoretical foundations and backgrounds of these two schools.</p>
<p>Suppose there is system<span class="math notranslate nohighlight">\(f_\theta\)</span>, <span class="math notranslate nohighlight">\(\theta\)</span> is a vector. The random variable <span class="math notranslate nohighlight">\(X\)</span> is generated from the system.</p>
<div class="math notranslate nohighlight">
\[
X \sim  p(x|\theta)
\]</div>
<p>How to estate the parameter <span class="math notranslate nohighlight">\(\theta\)</span> from the observation <span class="math notranslate nohighlight">\(x\)</span> from frequentist and Bayesian?</p>
</div>
<div class="section" id="frequentist">
<h2>2、Frequentist<a class="headerlink" href="#frequentist" title="Permalink to this headline">¶</a></h2>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/god-prospective.jpg"><img alt="_images/god-prospective.jpg" class="bg-primary mb-1 align-center" src="_images/god-prospective.jpg" style="width: 500px;" /></a>
<p>A frequentist believes that the world is determinstic. There is an ontology, and its true value is constant. Our goal is to determine this value or the range in which it lies. However, the true values are unknown. The only way to estimate true values are through the random phenomena that result from them.</p>
<p>In a jar with seven red balls and three blue balls, for example, if we randomly take out one ball and record it before putting it back in the jar. The probability that we will take out the red ball each time is <span class="math notranslate nohighlight">\(70\%\)</span>. The probability, no matter how the experiment is performed, is objective and unique. It is this objective fact that underpins the series of observations. Let us assume the unknown probability is <span class="math notranslate nohighlight">\(\theta\)</span> and we can estimate it. If we perform <span class="math notranslate nohighlight">\(100\)</span> experiments and get a red ball <span class="math notranslate nohighlight">\(72\)</span> times, we can intuitively estimate that it is <span class="math notranslate nohighlight">\(72\%\)</span> red, and since there are only <span class="math notranslate nohighlight">\(10\)</span> balls in the jar, we can determine that the most plausable estimation is that there are <span class="math notranslate nohighlight">\(7\)</span> red balls.</p>
<p>This is in fact an optimization problem with a maximum likelihood function.</p>
<p>Suppose the probability of getting out red ball is <span class="math notranslate nohighlight">\(\theta\)</span>, which is the Bernolli distribution:</p>
<div class="math notranslate nohighlight">
\[
p(x_i, \theta) = \theta^{x_i}(1-\theta)^{(1-x_i)}
\]</div>
<p><span class="math notranslate nohighlight">\(x_i = 1\)</span> indicates that we get read ball, and <span class="math notranslate nohighlight">\(x_i = 0\)</span> for blue ball</p>
<p>Suppose we perform <span class="math notranslate nohighlight">\(n\)</span> experiments. In theory, <span class="math notranslate nohighlight">\(\theta\)</span> can take any value between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>, including <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>. This is <span class="math notranslate nohighlight">\(\theta\in[0, 1]\)</span>. The <span class="math notranslate nohighlight">\(n\)</span> experiments can produce arbitrary permutations of length <span class="math notranslate nohighlight">\(n\)</span> sequence with elements of <span class="math notranslate nohighlight">\(0\)</span>s and <span class="math notranslate nohighlight">\(1\)</span>s. The total number of sequences is the permutations is <span class="math notranslate nohighlight">\(2^n\)</span>. But, after one round of our experiments, we only get one of <span class="math notranslate nohighlight">\(2^n\)</span> possibility in the sequence <span class="math notranslate nohighlight">\(0,1\)</span>.</p>
<p>Why did we get this sequence and not any other in one experiment? It is the physical reality that there are <span class="math notranslate nohighlight">\(10\)</span> balls in the jar, <span class="math notranslate nohighlight">\(7\)</span> of which are red balls which makes the observation happened. This objective reality determines <span class="math notranslate nohighlight">\(\theta\)</span>, and <span class="math notranslate nohighlight">\(\theta\)</span> in turn determines which sequence is most likely to be observed. That is, <span class="math notranslate nohighlight">\(\theta\)</span> makes the most likely sequence to occur. Represent this idea using mathematical formulation:</p>
<div class="math notranslate nohighlight">
\[
\theta = argmax_\theta\prod_{i=1}^N\theta^{x_i}(1-\theta)^{1-x_i}
\]</div>
<p>For ease of calculation, take the logarithm of the above equation：</p>
<div class="math notranslate nohighlight">
\[
\theta = \underset{\theta}{\operatorname{argmax}}\sum_{i=1}^Nlog(\theta^{x_i}(1-\theta)^{1-x_i}) = \underset{\theta}{\operatorname{argmax}}(-\sum_{i=1}^Nlog(\theta^{x_i}(1-\theta)^{1-x_i}))
\]</div>
<p>Let <span class="math notranslate nohighlight">\(L=-\sum_{i=1}^Nlog(\theta^{x_i}(1-\theta)^{1-x_i})\)</span>, and we calculate the derivative of <span class="math notranslate nohighlight">\(L\)</span> with respect to <span class="math notranslate nohighlight">\(\theta\)</span>：</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial \theta} = -\sum_{i=1}^N(\frac{x_i}{\theta} + (1-x_i)\frac{-1}{1-\theta}) = 0
\]</div>
<p>We get:</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = \frac{\sum_i^N x_i}{N}
\]</div>
</div>
<div class="section" id="bayesian">
<h2>3. Bayesian<a class="headerlink" href="#bayesian" title="Permalink to this headline">¶</a></h2>
<a class="bg-primary mb-1 reference internal image-reference" href="_images/bayesian.jpg"><img alt="_images/bayesian.jpg" class="bg-primary mb-1 align-center" src="_images/bayesian.jpg" style="width: 500px;" /></a>
<p>The Bayesian does not attempt to say that ‘events themselves are random’, or that ‘the world is somehow random in its ontology’. It starts from the point of ‘imperfect knowledge of the observer’ and constructs a framework for making inferences based on uncertain of the knowledge.</p>
<p>The random event in the eyes of frequentist is not random any more in the view of Bayesian, but is only unknown to the observer. So, the observer makes inferring from the observed evidence. The randomness is not arising from whether the event itself occurred or not, but merely describes the state of the observer’s knowledge of the event.</p>
<p>Bayesian probabilities are based on limited knowledge, while frequentists describe the ontology. An observer’s knowledge is updated when a new observation is made according to a Bayesian theorem. In Bayesian probability theory, it is assumed that the observer has limited knowledge of the event (for instance, Tom believes <em>a priori</em> that a coin is even based on his daily observations and feelings). Once the observer gets new observations (Tom tosses the coin over and over again and discovers that out of 100 tosses, only 20 come up heads), that will affect the observer’s original beliefs in the form of logical uncertainty (Tom doubts the coin is even and even begins to conclude it is not even). Because incomplete information prevents the observer from relying on simple logic to form inferences, he must turn to plausible reasoning, which assigns plausibility to a range of possible outcomes.</p>
<p>By way of example, Bayesian analysis describes the above process as the observer holding a particular prior belief, gaining new evidence through observation, and combining the new observed evidence and prior knowledge to arrive at a posterior belief, which reflects an updated state of knowledge. Bayesian probabilistic inference is concerned with building a logical system from incomplete knowledge and an assertion as a measure of plausibility.  An observer’s beliefs or knowledge about a variable is called a probability distribution by a frequentist. In a Bayesian approach, representations of human knowledge are constructed rather than representations of the objective world.  Bayesian probabilistic inference is, therefore, in many cases a better approach to solving the problem of observer inference in machine learning and bypasses the discussion about ontology.</p>
<p>The mathematical representation of the above discussion is:</p>
<div class="math notranslate nohighlight">
\[
p(\theta|x) = \frac{p(x|\theta)p(\theta)}{p(x)}
\]</div>
<p>where,</p>
<div class="math notranslate nohighlight">
\[
p(x) = \int p(x|\theta)p(\theta)d\theta
\]</div>
<p>By using the likelihood function, Bayes’ theorem relates the prior probability to the posterior probability. We can get the maximum posterior probability：</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = \theta_{max} = \underset{\theta}{\operatorname{argmax}} p(\theta|x)
= \underset{\theta}{\operatorname{argmax}}\frac{p(x|\theta)p(\theta)}{p(x)} 
= \underset{\theta}{\operatorname{argmax}} p(x|\theta)p(\theta)
\]</div>
<p>The powerful and fascinating part of Bayesian reasoning is that we start with subjective <em>a priori</em> beliefs and acquire an objective knowledge of the world by objective iterative observation. For example, we can obtain the posterior probabilities by combining evidence and prior probabilities. However, if we receive new evidence again, we can update our posterior probabilities by combining the previously obtained posterior probabilities with the new evidence. It can be is an iterative process.</p>
<p>Below I will use one simple example to show the process of iterative.</p>
<p>Incidence of breast cancer <span class="math notranslate nohighlight">\(0.1\%\)</span> in the some place. If the person has breast cancer, the test positive is <span class="math notranslate nohighlight">\(90\%\)</span> and if the person has no breast cancer, the test nagative is <span class="math notranslate nohighlight">\(90\%\)</span>. Suppose, these is a woman, whose the first time test is positive. How much posibility she has breast cancer? How about if the second time her test is still positive? How do you make decision if you were the doctor?</p>
<p>If we have no information, we randomly take one person, who has the probability of <span class="math notranslate nohighlight">\(0.001\)</span> to have breast cancer. This is the prior probability. <span class="math notranslate nohighlight">\(p(c)=0.001\)</span>. The person has cancer and the test is positive, which means <span class="math notranslate nohighlight">\(p(+|c) = 0.9\)</span>. So <span class="math notranslate nohighlight">\(p(-|c) = 0.1\)</span>. If a person has no cancer, the <span class="math notranslate nohighlight">\(p(-|\bar c) = 0.9\)</span> and <span class="math notranslate nohighlight">\(p(-|\bar c) = 0.1\)</span>.</p>
<p>So, after the first test is positive.</p>
<div class="math notranslate nohighlight">
\[
p(c|+) = \frac{p(c)p(+|c)}{p(c)p(+|c) + p(\bar c)p(+|\bar c)} = \frac{0.001x0.9}{0.001x0.9 + 0.999x0.1} \approx 0.9\%
\]</div>
<p>So, the first test as positive only means that she has 0.9% probability to have cancer. Maybe at this time the doctor can’t confirm the woman has cancer.</p>
<p>How about if the second time is still positive?</p>
<p>We take posterior probability（<span class="math notranslate nohighlight">\(p(c|+) \approx 0.009\)</span>) as the prior probability of second time test. So, <span class="math notranslate nohighlight">\(p(c) = 0.009\)</span> now.</p>
<div class="math notranslate nohighlight">
\[
p(c|+) = \frac{p(c)p(+|c)}{p(c)p(+|c) + p(\bar c)p(+|\bar c)} = \frac{0.009x0.9}{0.009x0.9 + 0.991x0.1} \approx 7.6\%
\]</div>
<p>Maybe now the doctor stll can’t confirm the woman has cancer. She will need further test</p>
<p>This example shows the process of how to update our beliefs with new evidence using Bayesian reasoning.</p>
</div>
<div class="section" id="comments">
<h2>4. Comments<a class="headerlink" href="#comments" title="Permalink to this headline">¶</a></h2>
<p>We’ll get the method based on how we look at the problem. The frequentist believes that the parameters are objective and do not change, even if they are unknown. The optimization of the likelihood function based on the observations can sometimes produce very extreme results. The Bayesian, on the other hand, believes that all parameters have random values and thus have probability distributions. Bayesian estimates of posterior distributions based on prior knowledge combined with new evidence do not produce extreme results. Because all parameters are random variables with distributions, Bayesians can use some sampling algorithm (e.g., MCMC), making it easier to build complex models.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>